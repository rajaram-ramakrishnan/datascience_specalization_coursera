---
title: "Coursera Practical Machine Learning Project"
author: "Rajaram"
date: "04/10/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 

## Data

The data consists of a Training data and a Test data (to be used to validate the selected model).

## Data Loading and Pre-processing

We are going to use the testing data provided for final validation. Training dataset provided will be split to training and testing.

```{r data_load}
# set the URL for the download
UrlTrain <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
UrlTest  <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

train_in <- read.csv(url(UrlTrain))
valid_in <- read.csv(url(UrlTest))

dim(train_in)
dim(valid_in)
```

In this step, we are going to remove variables that are mostly NA, remove first few variables which doesn't have any impact with outcome and remove variables that are near zero-variance.


```{r pre_process}
NAvar <- sapply(train_in, function(x) { mean(is.na(x))}) > 0.90
train_in <- train_in[, NAvar==FALSE]
valid_in <- valid_in[, NAvar==FALSE]

train_in <- train_in[, -c(1:7)]
valid_in <- valid_in[, -c(1:7)]

library(caret)
NZV <- nearZeroVar(train_in)
train_in <- train_in[, -NZV]
valid_in <- valid_in[, -NZV]

dim(train_in)
dim(valid_in)
```

## Datasets for prediction

We prepare the data for prediction by splitting the provided training data into 70% training and 30% testing.

```{r data_pred}
set.seed(1278)
inTrain <- createDataPartition(train_in$classe, p=0.7, list=FALSE)
training <- train_in[inTrain,]
testing <-  train_in[-inTrain,]

dim(training)
dim(testing)
```

## Exploratory Data Analysis

We are going to check if there are any of the variables which has high correlation with classe variable.

```{r corr}
classe_ind <- which(names(training)=="classe")
cor_mat <- cor(training[ , -classe_ind] , as.numeric(training$classe))
bestCorrelations <- subset(as.data.frame(as.table(cor_mat)), abs(Freq)>0.25)
bestCorrelations
```

As per the above output, best correlations with classe are around 0.3.
We will visualize now to check if we can use simple linear predictions with highly correlated variables.

```{r eda}
library(Rmisc)
library(ggplot2)

p1 <- ggplot(training, aes(classe,pitch_forearm)) + 
  geom_boxplot(aes(fill=classe))

p2 <- ggplot(training, aes(classe, magnet_belt_y)) + 
  geom_boxplot(aes(fill=classe))

p3 <- ggplot(training, aes(classe,magnet_arm_x)) + 
  geom_boxplot(aes(fill=classe))

p4 <- ggplot(training, aes(classe,magnet_arm_y)) + 
  geom_boxplot(aes(fill=classe))

multiplot(p1,p2,p3,p4,cols=2)
```

From the above plot, we infer that separation of classes is not possible. 
Hence , we need to proceed with Model Selection.

## Model Selection

We use the following models to predict the outcome.

 1. Decision Trees
 2. Random Forest
 3. Generalized Boosting Models
 
## Prediction with Decison Trees

```{r DT_model}
set.seed(12345)
library(rpart)
DTModel <- rpart(classe ~ ., data=training, method="class")
library(rattle)
fancyRpartPlot(DTModel)
```

We run this model against test data to check the accuracy.

```{r DT_pred}
predictDT <- predict(DTModel,newdata = testing,type="class")
cmDT <- confusionMatrix(predictDT, testing$classe)
cmDT
```

Plot Matrix results

```{r DT_plot}
plot(cmDT$table, col = cmDT$byClass, 
     main = paste("Decision Tree - Accuracy =", round(cmDT$overall['Accuracy'], 4)))
```

We see that accuracy is low around **0.7127** and out-of-sample error is **0.2863**

## Prediction with Random Forest

```{r RF_model}
controlRF <- trainControl(method="cv", number=3, verboseIter=FALSE)
RFModel <- train(classe ~ ., data=training, method="rf", trControl=controlRF)
RFModel$finalModel
```

We run this model against test data to check the accuracy.

```{r RF_predict}
predictRF <- predict(RFModel,newdata = testing)
cmRF <- confusionMatrix(predictRF, testing$classe)
cmRF
```

We can see that accuracy rate is high with accuracy rate of **0.9922** and out-of-sample error rate is **0.0078**

Model Plots:

```{r RF_Plot}
plot(RFModel)
plot(cmRF$table, col = cmRF$byClass, main = paste("Random Forest Confusion Matrix: Accuracy =", round(cmRF$overall['Accuracy'], 4)))
```

## Prediction with Generalized Boosting Models

```{r GBM_model}
set.seed(12345)
controlGBM <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
GBMModel  <- train(classe ~ ., data=training, method = "gbm", trControl = controlGBM, verbose = FALSE)
print(GBMModel)
```
We run this model against test data to check the accuracy.

```{r GBM_predict}
predictGBM <- predict(GBMModel,newdata = testing)
cmGBM <- confusionMatrix(predictGBM, testing$classe)
cmGBM
```
We can see that accuracy rate is high with accuracy rate of **0.9624** and out-of-sample error rate is **0.0376**

Model Plots :

```{r GBM_plot}
plot(cmGBM$table, col = cmGBM$byClass, main = paste("Gradient Boosted Confusion Matrix: Accuracy =", round(cmGBM$overall['Accuracy'], 4)))
```

## Applying the best model to validation data

By comparing all the models accuracy rates, we chose random forest model to predict classe of validation data.

```{r best_model}
Results <- predict(RFModel, newdata=valid_in)
ValidationRes <- data.frame(problem_id = valid_in$problem_id, predicted = Results)
ValidationRes
```



